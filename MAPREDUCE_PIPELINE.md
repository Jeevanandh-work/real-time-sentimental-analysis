# ğŸš€ MapReduce Pipeline - Twitter Sentiment Analysis

## **Complete MapReduce Architecture**

This document explains how all MapReduce components connect together in the sentiment analysis pipeline.

---

## **ğŸ“Š Pipeline Flow Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MAPREDUCE SENTIMENT ANALYSIS PIPELINE            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: MongoDB Collection (tweets)
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MAPREDUCE TASKS                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  1. SENTIMENT DISTRIBUTION       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ ğŸ—ºï¸  MAP:    sentiment â†’ 1        â”‚
   â”‚ ğŸ”„ SHUFFLE: Group by sentiment   â”‚
   â”‚ ğŸ”´ REDUCE:  Sum counts per type  â”‚
   â”‚ OUTPUT: {positive, negative...}  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  2. TOP HASHTAGS                 â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ ğŸ—ºï¸  MAP:    hashtag â†’ 1          â”‚
   â”‚ ğŸ”„ SHUFFLE: Group by hashtag     â”‚
   â”‚ ğŸ”´ REDUCE:  Sum counts per tag   â”‚
   â”‚ OUTPUT: [(#tag1, count)...]      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  3. SENTIMENT OVER TIME          â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ ğŸ—ºï¸  MAP:    date â†’ {count, total}â”‚
   â”‚ ğŸ”„ SHUFFLE: Group by date        â”‚
   â”‚ ğŸ”´ REDUCE:  Calculate avg score  â”‚
   â”‚ OUTPUT: [(date, avg_sentiment)...]â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  4. TOP WORDS BY SENTIMENT       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ ğŸ—ºï¸  MAP:    word â†’ 1             â”‚
   â”‚ ğŸ”„ SHUFFLE: Group by word        â”‚
   â”‚ ğŸ”´ REDUCE:  Sum word counts      â”‚
   â”‚ OUTPUT: [(word, count)...]       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESULTS AGGREGATION & STORAGE                    â”‚
â”‚              MongoDB Collection: mapreduce_analysis_results          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VISUALIZATIONS                              â”‚
â”‚  â€¢ sentiment_distribution.png                                        â”‚
â”‚  â€¢ sentiment_trend.png                                               â”‚
â”‚  â€¢ word_comparison.png                                               â”‚
â”‚  â€¢ top_hashtags.png                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **ğŸ”Œ Connection Points**

### **1. Map Phase Connection**
```python
# All MAP functions emit key-value pairs:
# ğŸ—ºï¸ sentiment_distribution_map(doc) â†’ (sentiment, 1)
# ğŸ—ºï¸ top_hashtags_map(doc) â†’ (hashtag, 1)
# ğŸ—ºï¸ sentiment_over_time_map(interval) â†’ (date, {count, total})
# ğŸ—ºï¸ top_words_map(doc) â†’ (word, 1)

# Connected through map_phase() framework function:
def map_phase(documents, map_func):
    mapped_data = []
    for doc in documents:
        for key, value in map_func(doc):
            mapped_data.append((key, value))
    return mapped_data
```

### **2. Shuffle Phase Connection**
```python
# All mapped data goes through shuffle_phase():
def shuffle_phase(mapped_data):
    """Groups values by key"""
    shuffled = defaultdict(list)
    for key, value in mapped_data:
        shuffled[key].append(value)
    return shuffled

# Result: {key1: [v1, v2, ...], key2: [v3, v4, ...], ...}
```

### **3. Reduce Phase Connection**
```python
# All REDUCE functions process grouped data:
# ğŸ”´ sentiment_distribution_reduce(key, values) â†’ count
# ğŸ”´ top_hashtags_reduce(key, values) â†’ count
# ğŸ”´ sentiment_over_time_reduce(key, values) â†’ {avg_sentiment, count}
# ğŸ”´ top_words_reduce(key, values) â†’ count

# Connected through reduce_phase() framework function:
def reduce_phase(shuffled_data, reduce_func):
    reduced = {}
    for key, values in shuffled_data.items():
        reduced[key] = reduce_func(key, values)
    return reduced
```

---

## **ğŸ“‹ Task-by-Task Connection**

### **TASK 1: Sentiment Distribution**

**Purpose:** Count tweets by sentiment type

```python
# STEP 1: MAP PHASE
# Input: All documents from collection
# ğŸ—ºï¸ sentiment_distribution_map(doc):
#    For each doc: yield (doc["sentiment_label"], 1)
# Output: [("positive", 1), ("negative", 1), ("positive", 1), ...]

# STEP 2: SHUFFLE PHASE
# Grouping: {
#    "positive": [1, 1, 1, ...],
#    "negative": [1, 1, ...],
#    "neutral": [1, ...]
# }

# STEP 3: REDUCE PHASE
# ğŸ”´ sentiment_distribution_reduce("positive", [1,1,1,1,1])
#    â†’ Returns: 5
# ğŸ”´ sentiment_distribution_reduce("negative", [1,1,1])
#    â†’ Returns: 3
# ğŸ”´ sentiment_distribution_reduce("neutral", [1])
#    â†’ Returns: 1

# FINAL OUTPUT:
{
    "positive": 5,
    "negative": 3,
    "neutral": 1,
    "total": 9
}
```

---

### **TASK 2: Top Hashtags**

**Purpose:** Find most frequently used hashtags

```python
# STEP 1: MAP PHASE
# ğŸ—ºï¸ top_hashtags_map(doc):
#    For each hashtag in doc["hashtags"]: yield (hashtag.lower(), 1)
# Output: [("#ai", 1), ("#ml", 1), ("#ai", 1), ("#python", 1), ...]

# STEP 2: SHUFFLE PHASE
# Grouping: {
#    "ai": [1, 1, 1, 1, 1],
#    "ml": [1, 1, 1],
#    "python": [1, 1]
# }

# STEP 3: REDUCE PHASE
# ğŸ”´ top_hashtags_reduce("ai", [1,1,1,1,1])
#    â†’ Returns: 5
# ğŸ”´ top_hashtags_reduce("ml", [1,1,1])
#    â†’ Returns: 3
# ğŸ”´ top_hashtags_reduce("python", [1,1])
#    â†’ Returns: 2

# FINAL OUTPUT (sorted, top 10):
[
    ("#ai", 5),
    ("#ml", 3),
    ("#python", 2)
]
```

---

### **TASK 3: Sentiment Over Time**

**Purpose:** Track sentiment score changes over days/months

```python
# STEP 1: MAP PHASE
# ğŸ—ºï¸ sentiment_over_time_map(interval="day"):
#    For each doc: yield (date_key, {"count": 1, "total": sentiment_score})
# Output: [
#    ("2025-10-28", {"count": 1, "total": 0.8}),
#    ("2025-10-28", {"count": 1, "total": -0.5}),
#    ("2025-10-29", {"count": 1, "total": 0.6}),
#    ...
# ]

# STEP 2: SHUFFLE PHASE
# Grouping: {
#    "2025-10-28": [{"count": 1, "total": 0.8}, {"count": 1, "total": -0.5}],
#    "2025-10-29": [{"count": 1, "total": 0.6}]
# }

# STEP 3: REDUCE PHASE
# ğŸ”´ sentiment_over_time_reduce("2025-10-28", [{"count": 1, "total": 0.8}, 
#                                               {"count": 1, "total": -0.5}])
#    total_count = 1 + 1 = 2
#    total_score = 0.8 + (-0.5) = 0.3
#    avg = 0.3 / 2 = 0.15
#    â†’ Returns: {"avg_sentiment": 0.15, "count": 2}

# ğŸ”´ sentiment_over_time_reduce("2025-10-29", [{"count": 1, "total": 0.6}])
#    total_count = 1
#    total_score = 0.6
#    avg = 0.6 / 1 = 0.6
#    â†’ Returns: {"avg_sentiment": 0.6, "count": 1}

# FINAL OUTPUT (sorted by date):
[
    ("2025-10-28", {"avg_sentiment": 0.15, "count": 2}),
    ("2025-10-29", {"avg_sentiment": 0.6, "count": 1})
]
```

---

### **TASK 4: Top Words by Sentiment**

**Purpose:** Find most frequently used words in each sentiment category

```python
# STEP 1: MAP PHASE (For positive sentiment)
# ğŸ—ºï¸ top_words_map(doc):
#    For each word in doc["clean_text"]: yield (word.lower(), 1)
# Output: [("love", 1), ("great", 1), ("love", 1), ("awesome", 1), ...]

# STEP 2: SHUFFLE PHASE
# Grouping: {
#    "love": [1, 1, 1, 1],
#    "great": [1, 1, 1],
#    "awesome": [1, 1]
# }

# STEP 3: REDUCE PHASE
# ğŸ”´ top_words_reduce("love", [1, 1, 1, 1])
#    â†’ Returns: 4
# ğŸ”´ top_words_reduce("great", [1, 1, 1])
#    â†’ Returns: 3
# ğŸ”´ top_words_reduce("awesome", [1, 1])
#    â†’ Returns: 2

# FINAL OUTPUT (sorted, top 10):
[
    ("love", 4),
    ("great", 3),
    ("awesome", 2)
]
```

---

## **ğŸ”— Integration Points**

### **How Everything Connects:**

```python
# In mapreduce_aggregations.py:

def comprehensive_mapreduce_analysis():
    """Master orchestrator connecting all MapReduce tasks"""
    
    coll = get_collection()
    tweets = list(coll.find({}, {"_id": 0}))
    
    # ============ TASK 1: Sentiment Distribution ============
    dist_custom = sentiment_distribution_custom_mapreduce()
    # Internally calls:
    # - map_phase(docs, sentiment_distribution_map)
    # - shuffle_phase(mapped)
    # - reduce_phase(shuffled, sentiment_distribution_reduce)
    
    # ============ TASK 2: Top Hashtags ============
    hashtags_custom = top_hashtags_custom_mapreduce(15)
    # Internally calls:
    # - map_phase(docs, top_hashtags_map)
    # - shuffle_phase(mapped)
    # - reduce_phase(shuffled, top_hashtags_reduce)
    
    # ============ TASK 3: Sentiment Over Time ============
    trend_custom = sentiment_over_time_custom_mapreduce("day")
    # Internally calls:
    # - map_phase(docs, sentiment_over_time_map("day"))
    # - shuffle_phase(mapped)
    # - reduce_phase(shuffled, sentiment_over_time_reduce)
    
    # ============ TASK 4: Top Words ============
    pos_words = top_words_by_sentiment_custom_mapreduce("positive", 10)
    # Internally calls:
    # - map_phase(docs, top_words_map)
    # - shuffle_phase(mapped)
    # - reduce_phase(shuffled, top_words_reduce)
    
    # ============ Store Results ============
    results = {
        "timestamp": datetime.now().isoformat(),
        "custom_mapreduce_framework": {
            "sentiment_distribution": dist_custom,
            "top_hashtags": hashtags_custom,
            "sentiment_trend": trend_custom,
            "top_positive_words": pos_words
        }
    }
    
    result_coll = coll.database["mapreduce_analysis_results"]
    result_coll.insert_one(results)
    
    # ============ Visualize Results ============
    create_sentiment_bar_chart_custom(dist_custom)
    create_trend_chart_custom(trend_custom)
    create_word_comparison_chart_custom()
```

---

## **ğŸ“Š Data Flow Example**

### **Sample Input Data:**
```python
tweets = [
    {"sentiment_label": "positive", "text": "I love this", "clean_text": "love this", "hashtags": ["ai", "ml"]},
    {"sentiment_label": "positive", "text": "Great work", "clean_text": "great work", "hashtags": ["ai"]},
    {"sentiment_label": "negative", "text": "Hate it", "clean_text": "hate it", "hashtags": ["bad"]},
    {"sentiment_label": "positive", "text": "Awesome", "clean_text": "awesome", "hashtags": ["ai"]}
]
```

### **Processing Flow:**

```
INPUT
  â†“
MAP PHASE (Process each document)
  â”œâ”€ sentiment_distribution_map() â†’ [("positive", 1), ("positive", 1), ("negative", 1), ("positive", 1)]
  â”œâ”€ top_hashtags_map() â†’ [("ai", 1), ("ml", 1), ("ai", 1), ("bad", 1), ("ai", 1)]
  â”œâ”€ top_words_map() â†’ [("love", 1), ("great", 1), ("hate", 1), ("awesome", 1)]
  â””â”€ sentiment_over_time_map() â†’ [(date1, {...}), (date1, {...}), (date2, {...}), ...]
  â†“
SHUFFLE PHASE (Group by key)
  â”œâ”€ sentiment: {"positive": [1,1,1], "negative": [1]}
  â”œâ”€ hashtags: {"ai": [1,1,1], "ml": [1], "bad": [1]}
  â”œâ”€ words: {"love": [1], "great": [1], "hate": [1], "awesome": [1]}
  â””â”€ dates: {date1: [...], date2: [...]}
  â†“
REDUCE PHASE (Aggregate values)
  â”œâ”€ sentiment_reduce() â†’ {"positive": 3, "negative": 1}
  â”œâ”€ hashtags_reduce() â†’ {"ai": 3, "ml": 1, "bad": 1}
  â”œâ”€ words_reduce() â†’ {"love": 1, "great": 1, "hate": 1, "awesome": 1}
  â””â”€ dates_reduce() â†’ {date1: {avg: 0.5, count: 2}, ...}
  â†“
OUTPUT
  â†“
VISUALIZATIONS & STORAGE
```

---

## **ğŸ¯ How to Run the Complete Pipeline**

```bash
# Execute the complete MapReduce pipeline
python scripts/mapreduce_aggregations.py
```

**This runs:**
1. âœ… MongoDB MapReduce operations
2. âœ… Python functional MapReduce
3. âœ… Custom MapReduce framework (with explicit Map, Shuffle, Reduce)
4. âœ… All 4 MapReduce tasks in parallel
5. âœ… Stores results in MongoDB
6. âœ… Generates visualizations
7. âœ… Prints analysis to console

---

## **ğŸ“ˆ Result Structure**

```python
{
    "timestamp": "2025-10-30T10:30:45.123456",
    "mongodb_mapreduce": {
        "sentiment_distribution": {"positive": 100, "negative": 20, "neutral": 30, "total": 150},
        "top_hashtags": [("#ai", 45), ("#ml", 30), ...],
        "sentiment_trend": [{"date": "2025-10-28", "avg_sentiment": 0.5, ...}, ...]
    },
    "python_mapreduce": {
        "sentiment_distribution": {...},
        "top_hashtags": [...],
        "sentiment_trend": [...],
        "avg_tweet_length": [...]
    },
    "custom_mapreduce_framework": {
        "sentiment_distribution": {"positive": 100, ...},
        "top_hashtags": [("#ai", 45), ...],
        "sentiment_trend": [...],
        "top_positive_words": [("love", 45), ("great", 30), ...],
        "top_negative_words": [("hate", 20), ("bad", 15), ...],
        "top_neutral_words": [("news", 10), ("update", 8), ...]
    }
}
```

---

## **ğŸ”‘ Key Connections**

| Component | Input | Map Function | Shuffle | Reduce Function | Output |
|-----------|-------|--------------|---------|-----------------|--------|
| **Sentiment Distribution** | All tweets | `sentiment_distribution_map()` | Group by sentiment | `sentiment_distribution_reduce()` | Sentiment counts |
| **Top Hashtags** | Tweets with hashtags | `top_hashtags_map()` | Group by hashtag | `top_hashtags_reduce()` | Top N hashtags |
| **Sentiment Trend** | Tweets with date & score | `sentiment_over_time_map()` | Group by date | `sentiment_over_time_reduce()` | Avg sentiment per date |
| **Top Words** | Tweets by sentiment | `top_words_map()` | Group by word | `top_words_reduce()` | Top words per sentiment |

---

## **âœ¨ Summary**

The MapReduce pipeline connects all components through:
1. **Consistent Map Functions** - All emit key-value pairs
2. **Universal Shuffle Phase** - Groups all data by key
3. **Flexible Reduce Functions** - Aggregate grouped values
4. **Orchestrator Function** - `comprehensive_mapreduce_analysis()` ties everything together
5. **Storage & Visualization** - Results saved to MongoDB and visualized

**All MapReduce operations follow the same pattern:**
```
Input Data â†’ MAP (emit key-value pairs)
          â†’ SHUFFLE (group by key)  
          â†’ REDUCE (aggregate values)
          â†’ Output Results
```
